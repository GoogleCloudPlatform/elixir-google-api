# Copyright 2017 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# NOTE: This class is auto generated by the swagger code generator program.
# https://github.com/swagger-api/swagger-codegen.git
# Do not edit the class manually.

defmodule GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1TrainingInput do
  @moduledoc """
  Represents input parameters for a training job. When using the gcloud command to submit your training job, you can specify the input parameters as command-line arguments and/or in a YAML configuration file referenced from the --config command-line argument. For details, see the guide to &lt;a href&#x3D;\&quot;/ml-engine/docs/tensorflow/training-jobs\&quot;&gt;submitting a training job&lt;/a&gt;.

  ## Attributes

  - args ([String.t]): Optional. Command line arguments to pass to the program. Defaults to: `null`.
  - hyperparameters (GoogleCloudMlV1HyperparameterSpec): Optional. The set of Hyperparameters to tune. Defaults to: `null`.
  - jobDir (String.t): Optional. A Google Cloud Storage path in which to store training outputs and other data needed for training. This path is passed to your TensorFlow program as the &#39;--job-dir&#39; command-line argument. The benefit of specifying this field is that Cloud ML validates the path for use in training. Defaults to: `null`.
  - masterConfig (GoogleCloudMlV1ReplicaConfig): Optional. The configuration for your master worker.  You should only set &#x60;masterConfig.acceleratorConfig&#x60; if &#x60;masterType&#x60; is set to a Compute Engine machine type. Learn about [restrictions on accelerator configurations for training.](/ml-engine/docs/tensorflow/using-gpus#compute-engine-machine-types-with-gpu)  Set &#x60;masterConfig.imageUri&#x60; only if you build a custom image. Only one of &#x60;masterConfig.imageUri&#x60; and &#x60;runtimeVersion&#x60; should be set. Learn more about [configuring custom containers](/ml-engine/docs/distributed-training-containers). Defaults to: `null`.
  - masterType (String.t): Optional. Specifies the type of virtual machine to use for your training job&#39;s master worker.  The following types are supported:  &lt;dl&gt;   &lt;dt&gt;standard&lt;/dt&gt;   &lt;dd&gt;   A basic machine configuration suitable for training simple models with   small to moderate datasets.   &lt;/dd&gt;   &lt;dt&gt;large_model&lt;/dt&gt;   &lt;dd&gt;   A machine with a lot of memory, specially suited for parameter servers   when your model is large (having many hidden layers or layers with very   large numbers of nodes).   &lt;/dd&gt;   &lt;dt&gt;complex_model_s&lt;/dt&gt;   &lt;dd&gt;   A machine suitable for the master and workers of the cluster when your   model requires more computation than the standard machine can handle   satisfactorily.   &lt;/dd&gt;   &lt;dt&gt;complex_model_m&lt;/dt&gt;   &lt;dd&gt;   A machine with roughly twice the number of cores and roughly double the   memory of &lt;i&gt;complex_model_s&lt;/i&gt;.   &lt;/dd&gt;   &lt;dt&gt;complex_model_l&lt;/dt&gt;   &lt;dd&gt;   A machine with roughly twice the number of cores and roughly double the   memory of &lt;i&gt;complex_model_m&lt;/i&gt;.   &lt;/dd&gt;   &lt;dt&gt;standard_gpu&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;standard&lt;/i&gt; that   also includes a single NVIDIA Tesla K80 GPU. See more about   &lt;a href&#x3D;\&quot;/ml-engine/docs/tensorflow/using-gpus\&quot;&gt;using GPUs to   train your model&lt;/a&gt;.   &lt;/dd&gt;   &lt;dt&gt;complex_model_m_gpu&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;complex_model_m&lt;/i&gt; that also includes   four NVIDIA Tesla K80 GPUs.   &lt;/dd&gt;   &lt;dt&gt;complex_model_l_gpu&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;complex_model_l&lt;/i&gt; that also includes   eight NVIDIA Tesla K80 GPUs.   &lt;/dd&gt;   &lt;dt&gt;standard_p100&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;standard&lt;/i&gt; that   also includes a single NVIDIA Tesla P100 GPU.   &lt;/dd&gt;   &lt;dt&gt;complex_model_m_p100&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;complex_model_m&lt;/i&gt; that also includes   four NVIDIA Tesla P100 GPUs.   &lt;/dd&gt;   &lt;dt&gt;standard_v100&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;standard&lt;/i&gt; that   also includes a single NVIDIA Tesla V100 GPU.   &lt;/dd&gt;   &lt;dt&gt;large_model_v100&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;large_model&lt;/i&gt; that   also includes a single NVIDIA Tesla V100 GPU.   &lt;/dd&gt;   &lt;dt&gt;complex_model_m_v100&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;complex_model_m&lt;/i&gt; that   also includes four NVIDIA Tesla V100 GPUs.   &lt;/dd&gt;   &lt;dt&gt;complex_model_l_v100&lt;/dt&gt;   &lt;dd&gt;   A machine equivalent to &lt;i&gt;complex_model_l&lt;/i&gt; that   also includes eight NVIDIA Tesla V100 GPUs.   &lt;/dd&gt;   &lt;dt&gt;cloud_tpu&lt;/dt&gt;   &lt;dd&gt;   A TPU VM including one Cloud TPU. See more about   &lt;a href&#x3D;\&quot;/ml-engine/docs/tensorflow/using-tpus\&quot;&gt;using TPUs to train   your model&lt;/a&gt;.   &lt;/dd&gt; &lt;/dl&gt;  You may also use certain Compute Engine machine types directly in this field. The following types are supported:  - &#x60;n1-standard-4&#x60; - &#x60;n1-standard-8&#x60; - &#x60;n1-standard-16&#x60; - &#x60;n1-standard-32&#x60; - &#x60;n1-standard-64&#x60; - &#x60;n1-standard-96&#x60; - &#x60;n1-highmem-2&#x60; - &#x60;n1-highmem-4&#x60; - &#x60;n1-highmem-8&#x60; - &#x60;n1-highmem-16&#x60; - &#x60;n1-highmem-32&#x60; - &#x60;n1-highmem-64&#x60; - &#x60;n1-highmem-96&#x60; - &#x60;n1-highcpu-16&#x60; - &#x60;n1-highcpu-32&#x60; - &#x60;n1-highcpu-64&#x60; - &#x60;n1-highcpu-96&#x60;  See more about [using Compute Engine machine types](/ml-engine/docs/tensorflow/machine-types#compute-engine-machine-types).  You must set this value when &#x60;scaleTier&#x60; is set to &#x60;CUSTOM&#x60;. Defaults to: `null`.
  - maxRunningTime (String.t): Optional. The maximum job running time. The default is 7 days. Defaults to: `null`.
  - packageUris ([String.t]): Required. The Google Cloud Storage location of the packages with the training program and any additional dependencies. The maximum number of package URIs is 100. Defaults to: `null`.
  - parameterServerConfig (GoogleCloudMlV1ReplicaConfig): Optional. The configuration for parameter servers.  You should only set &#x60;parameterServerConfig.acceleratorConfig&#x60; if &#x60;parameterServerConfigType&#x60; is set to a Compute Engine machine type. [Learn about restrictions on accelerator configurations for training.](/ml-engine/docs/tensorflow/using-gpus#compute-engine-machine-types-with-gpu)  Set &#x60;parameterServerConfig.imageUri&#x60; only if you build a custom image for your parameter server. If &#x60;parameterServerConfig.imageUri&#x60; has not been set, AI Platform uses the value of &#x60;masterConfig.imageUri&#x60;. Learn more about [configuring custom containers](/ml-engine/docs/distributed-training-containers). Defaults to: `null`.
  - parameterServerCount (String.t): Optional. The number of parameter server replicas to use for the training job. Each replica in the cluster will be of the type specified in &#x60;parameter_server_type&#x60;.  This value can only be used when &#x60;scale_tier&#x60; is set to &#x60;CUSTOM&#x60;.If you set this value, you must also set &#x60;parameter_server_type&#x60;.  The default value is zero. Defaults to: `null`.
  - parameterServerType (String.t): Optional. Specifies the type of virtual machine to use for your training job&#39;s parameter server.  The supported values are the same as those described in the entry for &#x60;master_type&#x60;.  This value must be consistent with the category of machine type that &#x60;masterType&#x60; uses. In other words, both must be AI Platform machine types or both must be Compute Engine machine types.  This value must be present when &#x60;scaleTier&#x60; is set to &#x60;CUSTOM&#x60; and &#x60;parameter_server_count&#x60; is greater than zero. Defaults to: `null`.
  - pythonModule (String.t): Required. The Python module name to run after installing the packages. Defaults to: `null`.
  - pythonVersion (String.t): Optional. The version of Python used in training. If not set, the default version is &#39;2.7&#39;. Python &#39;3.5&#39; is available when &#x60;runtime_version&#x60; is set to &#39;1.4&#39; and above. Python &#39;2.7&#39; works with all supported &lt;a href&#x3D;\&quot;/ml-engine/docs/runtime-version-list\&quot;&gt;runtime versions&lt;/a&gt;. Defaults to: `null`.
  - region (String.t): Required. The Google Compute Engine region to run the training job in. See the &lt;a href&#x3D;\&quot;/ml-engine/docs/tensorflow/regions\&quot;&gt;available regions&lt;/a&gt; for AI Platform services. Defaults to: `null`.
  - runtimeVersion (String.t): Optional. The AI Platform runtime version to use for training. If not set, AI Platform uses the default stable version, 1.0. For more information, see the &lt;a href&#x3D;\&quot;/ml-engine/docs/runtime-version-list\&quot;&gt;runtime version list&lt;/a&gt; and &lt;a href&#x3D;\&quot;/ml-engine/docs/versioning\&quot;&gt;how to manage runtime versions&lt;/a&gt;. Defaults to: `null`.
  - scaleTier (String.t): Required. Specifies the machine types, the number of replicas for workers and parameter servers. Defaults to: `null`.
    - Enum - one of [BASIC, STANDARD_1, PREMIUM_1, BASIC_GPU, BASIC_TPU, CUSTOM]
  - workerConfig (GoogleCloudMlV1ReplicaConfig): Optional. The configuration for workers.  You should only set &#x60;workerConfig.acceleratorConfig&#x60; if &#x60;workerType&#x60; is set to a Compute Engine machine type. [Learn about restrictions on accelerator configurations for training.](/ml-engine/docs/tensorflow/using-gpus#compute-engine-machine-types-with-gpu)  Set &#x60;workerConfig.imageUri&#x60; only if you build a custom image for your worker. If &#x60;workerConfig.imageUri&#x60; has not been set, AI Platform uses the value of &#x60;masterConfig.imageUri&#x60;. Learn more about [configuring custom containers](/ml-engine/docs/distributed-training-containers). Defaults to: `null`.
  - workerCount (String.t): Optional. The number of worker replicas to use for the training job. Each replica in the cluster will be of the type specified in &#x60;worker_type&#x60;.  This value can only be used when &#x60;scale_tier&#x60; is set to &#x60;CUSTOM&#x60;. If you set this value, you must also set &#x60;worker_type&#x60;.  The default value is zero. Defaults to: `null`.
  - workerType (String.t): Optional. Specifies the type of virtual machine to use for your training job&#39;s worker nodes.  The supported values are the same as those described in the entry for &#x60;masterType&#x60;.  This value must be consistent with the category of machine type that &#x60;masterType&#x60; uses. In other words, both must be AI Platform machine types or both must be Compute Engine machine types.  If you use &#x60;cloud_tpu&#x60; for this value, see special instructions for [configuring a custom TPU machine](/ml-engine/docs/tensorflow/using-tpus#configuring_a_custom_tpu_machine).  This value must be present when &#x60;scaleTier&#x60; is set to &#x60;CUSTOM&#x60; and &#x60;workerCount&#x60; is greater than zero. Defaults to: `null`.
  """

  use GoogleApi.Gax.ModelBase

  @type t :: %__MODULE__{
          :args => list(any()),
          :hyperparameters =>
            GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1HyperparameterSpec.t(),
          :jobDir => any(),
          :masterConfig => GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1ReplicaConfig.t(),
          :masterType => any(),
          :maxRunningTime => any(),
          :packageUris => list(any()),
          :parameterServerConfig =>
            GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1ReplicaConfig.t(),
          :parameterServerCount => any(),
          :parameterServerType => any(),
          :pythonModule => any(),
          :pythonVersion => any(),
          :region => any(),
          :runtimeVersion => any(),
          :scaleTier => any(),
          :workerConfig => GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1ReplicaConfig.t(),
          :workerCount => any(),
          :workerType => any()
        }

  field(:args, type: :list)

  field(
    :hyperparameters,
    as: GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1HyperparameterSpec
  )

  field(:jobDir)
  field(:masterConfig, as: GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1ReplicaConfig)
  field(:masterType)
  field(:maxRunningTime)
  field(:packageUris, type: :list)

  field(
    :parameterServerConfig,
    as: GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1ReplicaConfig
  )

  field(:parameterServerCount)
  field(:parameterServerType)
  field(:pythonModule)
  field(:pythonVersion)
  field(:region)
  field(:runtimeVersion)
  field(:scaleTier)
  field(:workerConfig, as: GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1ReplicaConfig)
  field(:workerCount)
  field(:workerType)
end

defimpl Poison.Decoder, for: GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1TrainingInput do
  def decode(value, options) do
    GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1TrainingInput.decode(value, options)
  end
end

defimpl Poison.Encoder, for: GoogleApi.MachineLearning.V1.Model.GoogleCloudMlV1TrainingInput do
  def encode(value, options) do
    GoogleApi.Gax.ModelBase.encode(value, options)
  end
end
